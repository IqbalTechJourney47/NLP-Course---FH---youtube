{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ceb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "129fbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 4.16MB/s]\n",
      "2023-10-23 12:20:00 INFO: \"urdu\" is an alias for \"ur\"\n",
      "2023-10-23 12:20:00 INFO: Downloading default packages for language: ur (Urdu) ...\n",
      "2023-10-23 12:20:00 INFO: File exists: C:\\Users\\Muhammad Iqbal\\stanza_resources\\ur\\default.zip\n",
      "2023-10-23 12:20:01 INFO: Finished downloading models and saved to C:\\Users\\Muhammad Iqbal\\stanza_resources.\n",
      "2023-10-23 12:20:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 2.84MB/s]\n",
      "2023-10-23 12:20:03 INFO: Loading these models for language: ur (Urdu):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | udtb          |\n",
      "| pos       | udtb_nocharlm |\n",
      "| lemma     | udtb_nocharlm |\n",
      "| depparse  | udtb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2023-10-23 12:20:03 INFO: Using device: cpu\n",
      "2023-10-23 12:20:03 INFO: Loading: tokenize\n",
      "2023-10-23 12:20:03 INFO: Loading: pos\n",
      "2023-10-23 12:20:03 INFO: Loading: lemma\n",
      "2023-10-23 12:20:03 INFO: Loading: depparse\n",
      "2023-10-23 12:20:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('urdu')\n",
    "nlp = stanza.Pipeline('ur')\n",
    "doc = nlp('میرا نام رابعہ/ربیعہ اقبال ہے۔ میں لاہور، پاکستان میں رہتی ہوں۔ لاہور پاکستان کے مشہور شہروں میں سے ایک ہے۔')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c7ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"میرا\",\n",
      "      \"lemma\": \"میں\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Acc,Gen|Number=Sing|Person=1|PronType=Prs\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 4\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"نام\",\n",
      "      \"lemma\": \"نام\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 5,\n",
      "      \"end_char\": 8\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"رابعہ\",\n",
      "      \"lemma\": \"رابعہ\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNPC\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 9,\n",
      "      \"end_char\": 14\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"/ربیعہ\",\n",
      "      \"lemma\": \"/ربیعہ\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"start_char\": 14,\n",
      "      \"end_char\": 20\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"اقبال\",\n",
      "      \"lemma\": \"اقبال\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 21,\n",
      "      \"end_char\": 26\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"ہے\",\n",
      "      \"lemma\": \"ہے\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"xpos\": \"VM\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"cop\",\n",
      "      \"start_char\": 27,\n",
      "      \"end_char\": 29\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"۔\",\n",
      "      \"lemma\": \"۔\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"SYM\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 29,\n",
      "      \"end_char\": 30\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"میں\",\n",
      "      \"lemma\": \"میں\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 31,\n",
      "      \"end_char\": 34\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"لاہور\",\n",
      "      \"lemma\": \"لاہور\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Case=Acc|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 35,\n",
      "      \"end_char\": 40\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"،\",\n",
      "      \"lemma\": \"،\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"SYM\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 40,\n",
      "      \"end_char\": 41\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"پاکستان\",\n",
      "      \"lemma\": \"پاکستان\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Case=Acc|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 42,\n",
      "      \"end_char\": 49\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"میں\",\n",
      "      \"lemma\": \"میں\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"PSP\",\n",
      "      \"feats\": \"AdpType=Post\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 50,\n",
      "      \"end_char\": 53\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"رہتی\",\n",
      "      \"lemma\": \"رہ\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VM\",\n",
      "      \"feats\": \"Aspect=Imp|Gender=Fem|Number=Sing|VerbForm=Part|Voice=Act\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 54,\n",
      "      \"end_char\": 58\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"ہوں\",\n",
      "      \"lemma\": \"ہے\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"xpos\": \"VAUX\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"aux\",\n",
      "      \"start_char\": 59,\n",
      "      \"end_char\": 62\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"۔\",\n",
      "      \"lemma\": \"۔\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"SYM\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 62,\n",
      "      \"end_char\": 63\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"لاہور\",\n",
      "      \"lemma\": \"لاہور\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNPC\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 64,\n",
      "      \"end_char\": 69\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"پاکستان\",\n",
      "      \"lemma\": \"پاکستان\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Case=Acc|Gender=Masc|Number=Sing|Person=3\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"start_char\": 70,\n",
      "      \"end_char\": 77\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"کے\",\n",
      "      \"lemma\": \"کا\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"PSP\",\n",
      "      \"feats\": \"AdpType=Post|Case=Acc|Gender=Masc|Number=Sing\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 78,\n",
      "      \"end_char\": 80\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"مشہور\",\n",
      "      \"lemma\": \"مشہور\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Case=Acc\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 81,\n",
      "      \"end_char\": 86\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"شہروں\",\n",
      "      \"lemma\": \"شہر\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Case=Acc|Gender=Masc|Number=Plur|Person=3\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 87,\n",
      "      \"end_char\": 92\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"میں\",\n",
      "      \"lemma\": \"میں\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"PSP\",\n",
      "      \"feats\": \"AdpType=Post\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 93,\n",
      "      \"end_char\": 96\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"سے\",\n",
      "      \"lemma\": \"سے\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"PSP\",\n",
      "      \"feats\": \"AdpType=Post\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 97,\n",
      "      \"end_char\": 99\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"ایک\",\n",
      "      \"lemma\": \"ایک\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"QC\",\n",
      "      \"feats\": \"NumType=Card\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 100,\n",
      "      \"end_char\": 103\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"ہے\",\n",
      "      \"lemma\": \"ہے\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"xpos\": \"VM\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"cop\",\n",
      "      \"start_char\": 104,\n",
      "      \"end_char\": 106\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \"۔\",\n",
      "      \"lemma\": \"۔\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"SYM\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 106,\n",
      "      \"end_char\": 107\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2139c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 12:24:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 1.78MB/s]\n",
      "2023-10-23 12:24:43 INFO: Loading these models for language: ur (Urdu):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | udtb    |\n",
      "=======================\n",
      "\n",
      "2023-10-23 12:24:43 INFO: Using device: cpu\n",
      "2023-10-23 12:24:43 INFO: Loading: tokenize\n",
      "2023-10-23 12:24:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Sentence 1 tokens=============\n",
      "id: (1,)\\ttext: میرا\n",
      "id: (2,)\\ttext: نام\n",
      "id: (3,)\\ttext: رابعہ\n",
      "id: (4,)\\ttext: /ربیعہ\n",
      "id: (5,)\\ttext: اقبال\n",
      "id: (6,)\\ttext: ہے\n",
      "id: (7,)\\ttext: ۔\n",
      "==========Sentence 2 tokens=============\n",
      "id: (1,)\\ttext: میں\n",
      "id: (2,)\\ttext: لاہور\n",
      "id: (3,)\\ttext: ،\n",
      "id: (4,)\\ttext: پاکستان\n",
      "id: (5,)\\ttext: میں\n",
      "id: (6,)\\ttext: رہتی\n",
      "id: (7,)\\ttext: ہوں\n",
      "id: (8,)\\ttext: ۔\n",
      "==========Sentence 3 tokens=============\n",
      "id: (1,)\\ttext: لاہور\n",
      "id: (2,)\\ttext: پاکستان\n",
      "id: (3,)\\ttext: کے\n",
      "id: (4,)\\ttext: مشہور\n",
      "id: (5,)\\ttext: شہروں\n",
      "id: (6,)\\ttext: میں\n",
      "id: (7,)\\ttext: سے\n",
      "id: (8,)\\ttext: ایک\n",
      "id: (9,)\\ttext: ہے\n",
      "id: (10,)\\ttext: ۔\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Sentence Segmentation\n",
    "\n",
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='ur', processors='tokenize')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'==========Sentence {i+1} tokens=============')\n",
    "    print(*[f'id: {token.id}\\\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "726c36b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['میرا نام رابعہ/ربیعہ اقبال ہے۔', 'میں لاہور، پاکستان میں رہتی ہوں۔', 'لاہور پاکستان کے مشہور شہروں میں سے ایک ہے۔']\n"
     ]
    }
   ],
   "source": [
    "# sentence\n",
    "\n",
    "print([sentence.text for sentence in doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8357a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 12:24:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 3.84MB/s]\n",
      "2023-10-23 12:24:46 INFO: Loading these models for language: ur (Urdu):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | udtb    |\n",
      "=======================\n",
      "\n",
      "2023-10-23 12:24:46 INFO: Using device: cpu\n",
      "2023-10-23 12:24:46 INFO: Loading: tokenize\n",
      "2023-10-23 12:24:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Sentence 1 tokens=============\n",
      "id: (1,)\\ttext: میرا\n",
      "id: (2,)\\ttext: نام\n",
      "id: (3,)\\ttext: رابعہ\n",
      "id: (4,)\\ttext: /ربیعہ\n",
      "id: (5,)\\ttext: اقبال\n",
      "id: (6,)\\ttext: ہے\n",
      "id: (7,)\\ttext: ۔\n",
      "==========Sentence 2 tokens=============\n",
      "id: (1,)\\ttext: میں\n",
      "id: (2,)\\ttext: لاہور\n",
      "id: (3,)\\ttext: ،\n",
      "id: (4,)\\ttext: پاکستان\n",
      "id: (5,)\\ttext: میں\n",
      "id: (6,)\\ttext: رہتی\n",
      "id: (7,)\\ttext: ہوں\n",
      "id: (8,)\\ttext: ۔\n",
      "==========Sentence 3 tokens=============\n",
      "id: (1,)\\ttext: لاہور\n",
      "id: (2,)\\ttext: پاکستان\n",
      "id: (3,)\\ttext: کے\n",
      "id: (4,)\\ttext: مشہور\n",
      "id: (5,)\\ttext: شہروں\n",
      "id: (6,)\\ttext: میں\n",
      "id: (7,)\\ttext: سے\n",
      "id: (8,)\\ttext: ایک\n",
      "id: (9,)\\ttext: ہے\n",
      "id: (10,)\\ttext: ۔\n"
     ]
    }
   ],
   "source": [
    "# tokenize_pretokenized=True\n",
    "\n",
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='ur', processors='tokenize', tokenize_pretokenized=True)\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'==========Sentence {i+1} tokens=============')\n",
    "    print(*[f'id: {token.id}\\\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88e8cf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 12:24:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 2.93MB/s]\n",
      "2023-10-23 12:24:46 INFO: Loading these models for language: ur (Urdu):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | udtb    |\n",
      "=======================\n",
      "\n",
      "2023-10-23 12:24:46 INFO: Using device: cpu\n",
      "2023-10-23 12:24:46 INFO: Loading: tokenize\n",
      "2023-10-23 12:24:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Sentence 1 tokens=============\n",
      "id: (1,)\\ttext: میرا\n",
      "id: (2,)\\ttext: نام\n",
      "id: (3,)\\ttext: رابعہ\n",
      "id: (4,)\\ttext: /ربیعہ\n",
      "id: (5,)\\ttext: اقبال\n",
      "id: (6,)\\ttext: ہے\n",
      "id: (7,)\\ttext: ۔\n",
      "==========Sentence 2 tokens=============\n",
      "id: (1,)\\ttext: میں\n",
      "id: (2,)\\ttext: لاہور\n",
      "id: (3,)\\ttext: ،\n",
      "id: (4,)\\ttext: پاکستان\n",
      "id: (5,)\\ttext: میں\n",
      "id: (6,)\\ttext: رہتی\n",
      "id: (7,)\\ttext: ہوں\n",
      "id: (8,)\\ttext: ۔\n",
      "==========Sentence 3 tokens=============\n",
      "id: (1,)\\ttext: لاہور\n",
      "id: (2,)\\ttext: پاکستان\n",
      "id: (3,)\\ttext: کے\n",
      "id: (4,)\\ttext: مشہور\n",
      "id: (5,)\\ttext: شہروں\n",
      "id: (6,)\\ttext: میں\n",
      "id: (7,)\\ttext: سے\n",
      "id: (8,)\\ttext: ایک\n",
      "id: (9,)\\ttext: ہے\n",
      "id: (10,)\\ttext: ۔\n"
     ]
    }
   ],
   "source": [
    "# tokenize_pretokenized=True\n",
    "\n",
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='ur', processors='tokenize', tokenize_pretokenized=False)\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'==========Sentence {i+1} tokens=============')\n",
    "    print(*[f'id: {token.id}\\\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "900375c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 12:26:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 3.86MB/s]\n",
      "2023-10-23 12:26:53 INFO: Loading these models for language: ur (Urdu):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | udtb    |\n",
      "=======================\n",
      "\n",
      "2023-10-23 12:26:53 INFO: Using device: cpu\n",
      "2023-10-23 12:26:53 INFO: Loading: tokenize\n",
      "2023-10-23 12:26:53 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Sentence 1 tokens=============\n",
      "id: (1,)\\ttext: میرا\n",
      "id: (2,)\\ttext: نام\n",
      "id: (3,)\\ttext: رابعہ\n",
      "id: (4,)\\ttext: /ربیعہ\n",
      "id: (5,)\\ttext: اقبال\n",
      "id: (6,)\\ttext: ہے\n",
      "id: (7,)\\ttext: ۔\n",
      "==========Sentence 2 tokens=============\n",
      "id: (1,)\\ttext: میں\n",
      "id: (2,)\\ttext: لاہور\n",
      "id: (3,)\\ttext: ،\n",
      "id: (4,)\\ttext: پاکستان\n",
      "id: (5,)\\ttext: میں\n",
      "id: (6,)\\ttext: رہتی\n",
      "id: (7,)\\ttext: ہوں\n",
      "id: (8,)\\ttext: ۔\n",
      "==========Sentence 3 tokens=============\n",
      "id: (1,)\\ttext: لاہور\n",
      "id: (2,)\\ttext: پاکستان\n",
      "id: (3,)\\ttext: کے\n",
      "id: (4,)\\ttext: مشہور\n",
      "id: (5,)\\ttext: شہروں\n",
      "id: (6,)\\ttext: میں\n",
      "id: (7,)\\ttext: سے\n",
      "id: (8,)\\ttext: ایک\n",
      "id: (9,)\\ttext: ہے\n",
      "id: (10,)\\ttext: ۔\n"
     ]
    }
   ],
   "source": [
    "# Tokenization without Sentence Segmentation\n",
    "# tokenize_no_ssplit=False\n",
    "\n",
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='ur', processors='tokenize', tokenize_no_ssplit=False)\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'==========Sentence {i+1} tokens=============')\n",
    "    print(*[f'id: {token.id}\\\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec9571ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 12:31:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 3.78MB/s]\n",
      "2023-10-23 12:31:15 INFO: Loading these models for language: ur (Urdu):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | udtb    |\n",
      "=======================\n",
      "\n",
      "2023-10-23 12:31:15 INFO: Using device: cpu\n",
      "2023-10-23 12:31:15 INFO: Loading: tokenize\n",
      "2023-10-23 12:31:15 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: میرا\twords: میرا\n",
      "token: نام\twords: نام\n",
      "token: رابعہ\twords: رابعہ\n",
      "token: /ربیعہ\twords: /ربیعہ\n",
      "token: اقبال\twords: اقبال\n",
      "token: ہے\twords: ہے\n",
      "token: ۔\twords: ۔\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='ur', processors='tokenize')\n",
    "for token in doc.sentences[0].tokens:\n",
    "    print(f'token: {token.text}\\twords: {\", \".join([word.text for word in token.words])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d300ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 12:37:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 3.94MB/s]\n",
      "2023-10-23 12:37:30 INFO: Loading these models for language: ur (Urdu):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | udtb          |\n",
      "| pos       | udtb_nocharlm |\n",
      "| lemma     | udtb_nocharlm |\n",
      "| depparse  | udtb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2023-10-23 12:37:30 INFO: Using device: cpu\n",
      "2023-10-23 12:37:30 INFO: Loading: tokenize\n",
      "2023-10-23 12:37:30 INFO: Loading: pos\n",
      "2023-10-23 12:37:30 INFO: Loading: lemma\n",
      "2023-10-23 12:37:30 INFO: Loading: depparse\n",
      "2023-10-23 12:37:31 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: میرا\tupos: PRON\txpos: PRP\tfeats: Case=Acc,Gen|Number=Sing|Person=1|PronType=Prs\n",
      "word: نام\tupos: NOUN\txpos: NN\tfeats: Case=Nom|Gender=Masc|Number=Sing|Person=3\n",
      "word: رابعہ\tupos: PROPN\txpos: NNPC\tfeats: Case=Nom|Gender=Masc|Number=Sing|Person=3\n",
      "word: /ربیعہ\tupos: PROPN\txpos: NNP\tfeats: Case=Nom|Gender=Masc|Number=Sing|Person=3\n",
      "word: اقبال\tupos: PROPN\txpos: NNP\tfeats: Case=Nom|Gender=Masc|Number=Sing|Person=3\n",
      "word: ہے\tupos: AUX\txpos: VM\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
      "word: ۔\tupos: PUNCT\txpos: SYM\tfeats: _\n",
      "word: میں\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: لاہور\tupos: PROPN\txpos: NNP\tfeats: Case=Acc|Gender=Masc|Number=Sing|Person=3\n",
      "word: ،\tupos: PUNCT\txpos: SYM\tfeats: _\n",
      "word: پاکستان\tupos: PROPN\txpos: NNP\tfeats: Case=Acc|Gender=Masc|Number=Sing|Person=3\n",
      "word: میں\tupos: ADP\txpos: PSP\tfeats: AdpType=Post\n",
      "word: رہتی\tupos: VERB\txpos: VM\tfeats: Aspect=Imp|Gender=Fem|Number=Sing|VerbForm=Part|Voice=Act\n",
      "word: ہوں\tupos: AUX\txpos: VAUX\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "word: ۔\tupos: PUNCT\txpos: SYM\tfeats: _\n",
      "word: لاہور\tupos: PROPN\txpos: NNPC\tfeats: Case=Nom|Gender=Masc|Number=Sing|Person=3\n",
      "word: پاکستان\tupos: PROPN\txpos: NNP\tfeats: Case=Acc|Gender=Masc|Number=Sing|Person=3\n",
      "word: کے\tupos: ADP\txpos: PSP\tfeats: AdpType=Post|Case=Acc|Gender=Masc|Number=Sing\n",
      "word: مشہور\tupos: ADJ\txpos: JJ\tfeats: Case=Acc\n",
      "word: شہروں\tupos: NOUN\txpos: NN\tfeats: Case=Acc|Gender=Masc|Number=Plur|Person=3\n",
      "word: میں\tupos: ADP\txpos: PSP\tfeats: AdpType=Post\n",
      "word: سے\tupos: ADP\txpos: PSP\tfeats: AdpType=Post\n",
      "word: ایک\tupos: NUM\txpos: QC\tfeats: NumType=Card\n",
      "word: ہے\tupos: AUX\txpos: VM\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
      "word: ۔\tupos: PUNCT\txpos: SYM\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='ur', processors='tokenize, pos, lemma, depparse')\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}'\n",
    "        for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae6f33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 12:46:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 4.14MB/s]\n",
      "2023-10-23 12:46:04 INFO: Loading these models for language: ur (Urdu):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | udtb          |\n",
      "| pos       | udtb_nocharlm |\n",
      "| lemma     | udtb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2023-10-23 12:46:04 INFO: Using device: cpu\n",
      "2023-10-23 12:46:04 INFO: Loading: tokenize\n",
      "2023-10-23 12:46:04 INFO: Loading: pos\n",
      "2023-10-23 12:46:05 INFO: Loading: lemma\n",
      "2023-10-23 12:46:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: میرا\thead id: 2\thead: نام\tdeprel: nmod\n",
      "id: 2\tword: نام\thead id: 5\thead: اقبال\tdeprel: nsubj\n",
      "id: 3\tword: رابعہ\thead id: 4\thead: /ربیعہ\tdeprel: compound\n",
      "id: 4\tword: /ربیعہ\thead id: 5\thead: اقبال\tdeprel: nmod\n",
      "id: 5\tword: اقبال\thead id: 0\thead: root\tdeprel: root\n",
      "id: 6\tword: ہے\thead id: 5\thead: اقبال\tdeprel: cop\n",
      "id: 7\tword: ۔\thead id: 5\thead: اقبال\tdeprel: punct\n",
      "id: 1\tword: میں\thead id: 6\thead: رہتی\tdeprel: nsubj\n",
      "id: 2\tword: لاہور\thead id: 6\thead: رہتی\tdeprel: obl\n",
      "id: 3\tword: ،\thead id: 2\thead: لاہور\tdeprel: punct\n",
      "id: 4\tword: پاکستان\thead id: 6\thead: رہتی\tdeprel: obl\n",
      "id: 5\tword: میں\thead id: 4\thead: پاکستان\tdeprel: case\n",
      "id: 6\tword: رہتی\thead id: 0\thead: root\tdeprel: root\n",
      "id: 7\tword: ہوں\thead id: 6\thead: رہتی\tdeprel: aux\n",
      "id: 8\tword: ۔\thead id: 6\thead: رہتی\tdeprel: punct\n",
      "id: 1\tword: لاہور\thead id: 2\thead: پاکستان\tdeprel: compound\n",
      "id: 2\tword: پاکستان\thead id: 5\thead: شہروں\tdeprel: nmod\n",
      "id: 3\tword: کے\thead id: 2\thead: پاکستان\tdeprel: case\n",
      "id: 4\tword: مشہور\thead id: 5\thead: شہروں\tdeprel: amod\n",
      "id: 5\tword: شہروں\thead id: 8\thead: ایک\tdeprel: obl\n",
      "id: 6\tword: میں\thead id: 5\thead: شہروں\tdeprel: case\n",
      "id: 7\tword: سے\thead id: 5\thead: شہروں\tdeprel: case\n",
      "id: 8\tword: ایک\thead id: 0\thead: root\tdeprel: root\n",
      "id: 9\tword: ہے\thead id: 8\thead: ایک\tdeprel: cop\n",
      "id: 10\tword: ۔\thead id: 8\thead: ایک\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ur', processors='tokenize, pos, lemma')\n",
    "# doc = nlp('Nous avons atteint la fin du sentier.')\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}'\n",
    "        for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb39d8",
   "metadata": {},
   "source": [
    "Stanza – A Python NLP Package for Many Human Languages\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507d711",
   "metadata": {},
   "source": [
    "Muti-Word Token (MWT) not available for Urdu\n",
    "\n",
    "Named Entity Recognition (NER) not available for Urdu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6f21d",
   "metadata": {},
   "source": [
    "https://stanfordnlp.github.io/stanza/data_objects.html#document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48df95e",
   "metadata": {},
   "source": [
    "http://stanza.run/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04aed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
